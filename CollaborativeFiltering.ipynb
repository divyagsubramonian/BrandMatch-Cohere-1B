{"cells":[{"cell_type":"markdown","metadata":{"id":"f9qyEWE1Sz6C"},"source":["### IMPLEMENTATION"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sdrwmcuoRoVj"},"outputs":[],"source":["# Connecting spreadsheet to Colab notebook\n","\n","from google.colab import auth\n","auth.authenticate_user()\n","\n","import gspread\n","from google.auth import default\n","creds, _ = default()\n","\n","gc = gspread.authorize(creds)\n","import pandas as pd\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YmSUzYM4ThMp"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import json\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import roc_auc_score\n","from sklearn.feature_extraction.text import TfidfVectorizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WEAwZH_OTjTW"},"outputs":[],"source":["# Open spreadsheet and transfer data\n","\n","#import pandas as pd\n","#import os\n","\n","#url = \"https://docs.google.com/spreadsheets/d/1r9qTAWrgrN6YX2PFJ7pkOLpAIXfLeVVKOgrxfOQtdlg/edit?usp=sharing\"\n","#df = pd.read_csv(url)\n","\n","#check if the spreadsheet opened right\n","#try:\n","#  sh = gc.open_by_url(\"https://docs.google.com/spreadsheets/d/1r9qTAWrgrN6YX2PFJ7pkOLpAIXfLeVVKOgrxfOQtdlg/edit?usp=sharing\")\n","#  print(\"Spreadsheet opened properly\", sh.title)\n","#except Exception as e:\n","#  print(\"Spreadsheet did not open correctly: {e}\", sh.title)\n","\n","#check if we can get data from our spreadsheet\n","#try:\n","#  rows = sh.get_all_values()\n","#  print(rows)\n","#except AttributeError as e:\n","#  print(\"Data not accessed: {e}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"elapsed":451,"status":"error","timestamp":1733528653427,"user":{"displayName":"Sophia Smith","userId":"17476007756299720550"},"user_tz":480},"id":"lgOFyksWTm5j","outputId":"62330b14-7177-480b-d43b-cb5f43db2252"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'sh' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-a4fec599e9eb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mworksheet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworksheet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'AI Challenge Dataset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworksheet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'sh' is not defined"]}],"source":["worksheet = sh.worksheet('AI Challenge Dataset')\n","rows = worksheet.get_all_values()\n","\n","df = pd.DataFrame.from_records(rows)\n","\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YBs1WZUCTptu"},"outputs":[],"source":["cols = list(df.columns)\n","print(cols)\n","\n","df = df.rename(columns={0: 'brand_id', 1: 'review_id', 2: 'review_dating_by_category', 3: 'ai_aspect', 4: 'ai_sentiment', 5: 'avg_rating', 6: 'is_product_recommended', 7: 'review_content', 8: 'is_anonymous', 9: 'like_count', 10: 'display_name'})\n","new_cols = list(df.columns)\n","print(new_cols)\n","\n","df = df.drop(axis=0, index=0)\n","\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N2CtbWxLT58F"},"outputs":[],"source":["df = df.dropna()\n","\n","df = df.dropna(how='all')\n","df = df.dropna(axis=1)\n","df = df.dropna(axis=1, how='all')\n","\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3zxBUi0Ooss1"},"outputs":[],"source":["import pandas as pd\n","import json\n","\n","df_sub = df[['review_dating_by_category']]\n","df_sub['review_dating_by_category'] = df_sub['review_dating_by_category'].apply(json.loads)\n","df_exploded = df_sub.explode('review_dating_by_category').reset_index(drop=True)\n","df_normalized = pd.json_normalize(df_exploded['review_dating_by_category'])\n","df_final = df_normalized[['id', 'review_id', 'rating_score', 'rating_category']]\n","\n","df_final.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DuRgfzrmUIRf"},"outputs":[],"source":["df_pivot_by_review_id = df_final.pivot_table(index='review_id', columns='rating_category', values='rating_score', aggfunc='first').reset_index()\n","df_pivot_by_review_id.fillna(0, inplace=True)\n","df_pivot_by_review_id.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lQ30oKKph852"},"outputs":[],"source":["# RUN THIS CELL ONCE (TO SAVE TO YOUR DRIVE INDIVIDUALLY)\n","\n","#saving cleaned data to pickle file\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","df.to_pickle('/content/drive/My Drive/df_pivot_by_review_id.pkl')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eFfvVN8ZiENI"},"outputs":[],"source":["cleaned_data = pd.read_pickle('/content/drive/MyDrive/df_pivot_by_review_id.pkl')\n","cleaned_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FWpTyRf3ndQe"},"outputs":[],"source":["#Inspecting EDA results\n","#import pickle\n","#from google.colab import drive\n","# Load the pickle file from Google Drive\n","\n","#with open('/content/drive/MyDrive/df_pivot_by_review_id.pkl', 'rb') as file:\n","#    loaded_eda_results = pickle.load(file)\n","\n","# Inspect the loaded data\n","#print(loaded_eda_results.keys())  # Shows the keys of the dictionary\n","#print(loaded_eda_results['missing_values'])  # Example: Access missing values\n"]},{"cell_type":"markdown","metadata":{"id":"dUJhPCalrDa0"},"source":["# ITEM-ITEM COLLABORATIVE FILTERING\n","- using our cleaned data, calling it by pickle, we will use this to find patterns through an algorithm called collaborative filtering\n","\n","Here's an explanation of what I did:\n","\n","## 1. Data Prep:\n","- Extracts specific rating columns (qualities) and normalizes them to a [0, 1] range for clustering and similarity calculations.\n","-Ensures the clustering process isn't biased by varying scales across columns.\n","\n","## 2. Using Clustering with K-Means\n","- Determines the optimal number of clusters by calculating the \"inertia\" (sum of squared distances to the nearest cluster center) for different values of k.\n","- Using the elbow method to visually identify the optimal k.\n","- Assigns each product to one of the 3 clusters (optimal_k=3) based on their qualities.\n","- finally, i plotted the `inertia` vs number of clusters and used a heatmap visual to show/determine 'elbow point' or most optimal k\n","\n","## 3. Cluster Analysis\n","- Computes the mean ratings for each quality `numeric_columns` within each cluster to summarize the clusters.\n","- this analysis provides a quick way to inpret and see what clusters represent and its patterns\n","\n","## 4. Dimensionality Reduction\n","- Reduces the 6-dimensional quality space to 2 dimensions for easier visualization\n","- Helps visually interpret cluster separability.\n","- Visualizes clusters in 2D space, showing which items belong to which cluster.\n","\n","## 5. Cosine Similarity\n","- Calculates item-to-item similarity for each cluster using cosine similarity.\n","- Ensures similarity is computed only within clusters for higher relevance.\n","- i convert the similarity matrix into a DataFrame for better interpretation and usability.\n","\n","## 6. Some Sample Recommendations:\n","- Recommends top n items similar to a given item (item) within a specified cluster\n","- Uses the precomputed similarity matrix for efficiency\n","\n","## 6a. Cross Cluster Recs\n","\n","- Recommends similar items across clusters, excluding the current cluster--> flexibility to refine recommendations by weighing similarities across clusters.\n","\n","## 6b. Reccommendation for all items\n","\n","- Automates the process of generating recommendations for all items in a cluster.\n","\n","## 7. Visualizers\n","- Visualizes item-to-item similarities within each cluster\n","- This makes it easier to interpret how qualities relate to each other\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5eE8e6KzrAZ9"},"outputs":[],"source":["from sklearn.metrics.pairwise import cosine_similarity\n","import pandas as pd\n","from sklearn.cluster import KMeans\n","import matplotlib.pyplot as plt\n","from sklearn.decomposition import PCA\n","import seaborn as sns\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import silhouette_score, pairwise_distances\n","\n","\n","#Utility Functions\n","####################################################################################################################################\n","\n","#Function name: normalize_data\n","#Function Purpose: normalize the data using a minmaxscalar\n","#In item-to-item collaborative filtering, \"normalizing data\" means adjusting the rating values of users on different items so that they are on a comparable scale\n","def normalize_data(group, columns):\n","    scaler = MinMaxScaler()\n","    normalized = pd.DataFrame(scaler.fit_transform(group[columns]), columns=columns, index=group.index)\n","    return normalized\n","\n","####################################################################################################################################\n","#Function name: determine optimal clusters\n","#Function Purpose: determine the number of clusters using approach of the silhouette method to be more programmatic\n","def determine_optimal_clusters(data, max_clusters=10):\n","  inertia = []\n","  K = range(2, max_clusters+1)\n","  silhouette_scores = []\n","  for k in K:\n","      kmeans = KMeans(n_clusters=k, random_state=42)\n","      kmeans.fit(qualities_normalized)\n","      inertia.append(kmeans.inertia_)\n","      silhouette_scores.append(silhouette_score(qualities_normalized, kmeans.labels_))\n","\n","  #Plot the elbow curve\n","  plt.figure(figsize=(8, 5))\n","  plt.plot(K, inertia, 'bx-', label='Inertia')\n","  plt.xlabel('Number of Clusters (K)')\n","  plt.ylabel('Inertia')\n","  plt.title('Elbow Method for Optimal K')\n","\n","  # Plot the silhouette curve\n","  plt.figure(figsize=(8, 5))\n","  plt.plot(K, silhouette_scores, 'go-')\n","  plt.xlabel('Number of Clusters (K)')\n","  plt.ylabel('Silhouette Score')\n","  plt.title('Silhouette Score for Optimal K')\n","  plt.show()\n","\n","  # finding optimal_k\n","  optimal_k = K[silhouette_scores.index(max(silhouette_scores))]\n","  print(f\"Optimal number of clusters: {optimal_k}\")\n","  return optimal_k\n","\n","####################################################################################################################################\n","#Function name: perform_clustering\n","#Function Purpose: Apply KMeans with optimal clusters and returns labels\n","def perform_clustering(data, n_clusters):\n","  kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n","  labels = kmeans.fit_predict(data)\n","  return kmeans, labels\n","\n","####################################################################################################################################\n","#Function name:\n","#Function Purpose: compute cosine similarity for items in a cluster\n","def compute_cosine_similarity(group, numeric_columns, metric='cosine'):\n","    #transpose the qualities to treat them as items\n","    qualities_transposed = group[numeric_columns].T\n","    # Compute cosine similarity for qualities within the cluster\n","    if metric == 'cosine':\n","        similarity_matrix = 1 - pairwise_distances(qualities_transposed, metric='cosine')\n","    elif metric == 'euclidean':\n","        similarity_matrix = 1 / (1 + pairwise_distances(qualities_transposed, metric='euclidean'))\n","    elif metric == 'correlation':\n","        similarity_matrix = 1 - pairwise_distances(qualities_transposed, metric='correlation')\n","    else:\n","        raise ValueError(f\"Unsupported metric: {metric}\")\n","    similarity_df = pd.DataFrame(similarity_matrix, index=numeric_columns, columns=numeric_columns)\n","    return similarity_df\n","\n","####################################################################################################################################\n","#Function name: get_recommendations\n","#Function Purpose: Get top N similar items for a specific item in a cluster\n","def get_recommendations(cluster, item, cluster_similarity_results, n=3):\n","    try:\n","        similarity_df = cluster_similarity_results[cluster]\n","        if item not in similarity_df.columns:\n","            print(f\"Item '{item}' not found in cluster {cluster}.\")\n","            return pd.Series(dtype='float64')\n","        similar_items = similarity_df[item].sort_values(ascending=False)[1:n+1]  # Exclude itself\n","        return similar_items\n","    except KeyError:\n","        print(f\"Cluster {cluster} not found in similarity results.\")\n","        return pd.Series(dtype='float64')\n","\n","####################################################################################################################################\n","#Function name: plot_pca_clusters\n","#FunctionPurpose: reduces the dimensionality of `qualities_normalized` data (so from 6D to 2D space)\n","#the PCA components are stored in columns `PCA1` and `PCA2` of `df_pivot_by_review_id`, and theses columns are only used to show\n","#the clusters in a 2D space\n","def plot_pca_clusters(df, pca_components, labels_col):\n","  plt.figure(figsize=(10, 6))\n","  for cluster in df_pivot_by_review_id['Cluster'].unique():\n","      cluster_data = df_pivot_by_review_id[df_pivot_by_review_id['Cluster'] == cluster]\n","      plt.scatter(cluster_data['PCA1'], cluster_data['PCA2'], label=f'Cluster {cluster}')\n","  plt.xlabel('PCA1')\n","  plt.ylabel('PCA2')\n","  plt.title('Clusters of Product Qualities')\n","  plt.legend()\n","  plt.show()\n","\n","####################################################################################################################################\n","#Function name:\n","#Function Purpose: Plot a heatmap of similarity for a specific cluster\n","def plot_heatmap(cluster, cluster_similarity_results):\n","    if cluster in cluster_similarity_results:\n","        similarity_df = cluster_similarity_results[cluster]\n","        plt.figure(figsize=(8, 6))\n","        sns.heatmap(similarity_df, annot=True, cmap='coolwarm', fmt='.2f')\n","        plt.title(f\"Item Similarity Heatmap for Cluster {cluster}\")\n","        plt.show()\n","    else:\n","        print(f\"Cluster {cluster} not found in similarity results.\")\n","\n","####################################################################################################################################\n","\n","\n","# Main work\n","####################################################################################################################################\n","\n","# load Data\n","df_pivot_by_review_id.head()\n","qualities = df_pivot_by_review_id[['rating_packaging', 'rating_price', 'rating_quality',\n","                                   'rating_service', 'rating_shipping', 'rating_taste']]\n","\n","#Checking distribution of overall features\n","print(\"CHECKING DISTRIBUTION ............\")\n","print(\"Overall distribution of numeric columns:\")\n","print(qualities.describe())\n","\n","# normalize the data\n","qualities_normalized = normalize_data(qualities,columns=qualities.columns)\n","\n","#Just trying things out: Let's see what happens if I drop low-variance features\n","print(\"DROPPING LOW VARIANCE FEATURES ............\")\n","low_variance_columns = [col for col in qualities.columns if qualities[col].var() < 0.01]\n","if low_variance_columns:\n","    print(\"THE LOW VARIANCE DROPPING: \")\n","    print(f\"Dropping low-variance columns: {low_variance_columns}\")\n","    qualities_normalized = qualities_normalized.drop(columns=low_variance_columns)\n","\n","# determine optimal number of clusters\n","optimal_k = determine_optimal_clusters(qualities_normalized)\n","\n","# perform clustering\n","kmeans, labels = perform_clustering(qualities_normalized, n_clusters=optimal_k)\n","df_pivot_by_review_id['Cluster'] = labels\n","\n","# analyze the clusters\n","numeric_columns = qualities_normalized.columns\n","cluster_similarity_results = {}\n","for cluster, group in df_pivot_by_review_id.groupby('Cluster'):\n","    if len(group) <= 1 or group[numeric_columns].var().sum() == 0:\n","        print(f\"Cluster {cluster} has too few items or low variance.\")\n","        cluster_similarity_results[cluster] = pd.DataFrame(0, index=numeric_columns, columns=numeric_columns)\n","    else:\n","        cluster_similarity_results[cluster] = compute_cosine_similarity(group, numeric_columns)\n","\n","\n","# PCA for visualization (not cleaning the data/reducing dimensionality)\n","pca = PCA(n_components=2)\n","qualities_pca = pca.fit_transform(qualities_normalized)\n","df_pivot_by_review_id['PCA1'] = qualities_pca[:, 0]\n","df_pivot_by_review_id['PCA2'] = qualities_pca[:, 1]\n","plot_pca_clusters(df_pivot_by_review_id, ['PCA1', 'PCA2'], 'Cluster')\n","\n","# compute cosine similarity within clusters\n","cluster_similarity_results = {}\n","for cluster, group in df_pivot_by_review_id.groupby('Cluster'):\n","    if len(group) <= 1 or group[numeric_columns].var().sum() == 0:\n","        print(f\"Cluster {cluster} has too few items or low variance for similarity calculation.\")\n","        cluster_similarity_results[cluster] = pd.DataFrame(0, index=numeric_columns, columns=numeric_columns)\n","    else:\n","        normalized_group = normalize_data(group, numeric_columns)\n","        cluster_similarity_results[cluster] = compute_cosine_similarity(normalized_group, numeric_columns)\n","\n","# generate recommendations for all items in each cluster\n","all_cluster_recommendations = {}\n","for cluster in cluster_similarity_results.keys():\n","    print(f\"Generating recommendations for Cluster {cluster}...\")\n","    cluster_recommendations = {item: get_recommendations(cluster, item, cluster_similarity_results) for item in numeric_columns}\n","    all_cluster_recommendations[cluster] = cluster_recommendations\n","\n","# display all recommendations\n","print(\"\\nRecommendations for all qualities in Cluster 0:\")\n","for item, recommendations in all_cluster_recommendations[0].items():\n","    print(f\"Top 3 recommendations for '{item}':\")\n","    print(recommendations)\n","    print()\n","\n","# Generate heatmaps for all clusters\n","for cluster in cluster_similarity_results.keys():\n","    plot_heatmap(cluster, cluster_similarity_results)"]},{"cell_type":"markdown","metadata":{"id":"jWz1s6VXZN_a"},"source":["# Doing some modeling things type shi -\n","lol"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6LhAvDmuZNVI"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from textblob import TextBlob\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.cluster import KMeans\n","from sklearn.decomposition import LatentDirichletAllocation\n","import spacy\n","from spacy.training import Example\n","import random\n","import nltk\n","import re\n","\n","# Ensure required NLTK resources are downloaded\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","\n","# Sample DataFrame for reviews\n","df_reviews = pd.DataFrame({\n","    'review_content': [\n","        \"The packaging was secure and beautiful, but the price felt too high for the quality.\",\n","        \"The taste was amazing, and it arrived on time.\",\n","        \"The service was helpful, but the packaging was damaged.\",\n","        \"The shipping was fast, and the product was affordable.\",\n","        \"The quality of the item exceeded expectations, but the packaging was unattractive.\"\n","    ]\n","})\n","\n","### 1. AI Sentiment Analysis ###\n","# Predefined categories with associated keywords\n","categories = {\n","    \"packaging\": [\"packaging\", \"box\", \"wrap\", \"secure\", \"damaged\", \"design\"],\n","    \"quality\": [\"quality\", \"durable\", \"sturdy\", \"poorly made\"],\n","    \"taste\": [\"taste\", \"flavor\", \"delicious\", \"bland\"],\n","    \"service\": [\"service\", \"helpful\", \"rude\", \"customer support\"],\n","    \"shipping\": [\"shipping\", \"fast\", \"delayed\", \"broken\"],\n","    \"price\": [\"price\", \"affordable\", \"expensive\", \"worth\", \"value\"]\n","}\n","\n","# Categorize reviews based on keywords\n","def categorize_review(review):\n","    scores = {category: 0 for category in categories}\n","    for category, keywords in categories.items():\n","        if any(keyword in review.lower() for keyword in keywords):\n","            analysis = TextBlob(review)\n","            scores[category] = analysis.sentiment.polarity\n","    return scores\n","\n","# Apply categorization to the reviews\n","df_reviews['categorized_scores'] = df_reviews['review_content'].apply(categorize_review)\n","categorized_df = pd.json_normalize(df_reviews['categorized_scores'])\n","df_reviews = pd.concat([df_reviews, categorized_df], axis=1)\n","\n","# Visualize AI Sentiment\n","plt.figure(figsize=(8, 5))\n","categorized_df.mean().plot(kind='barh', color=sns.color_palette('muted'))\n","plt.xlabel('Average Sentiment Score')\n","plt.ylabel('Categories')\n","plt.title('AI Sentiment Analysis by Categories')\n","plt.gca().spines[['top', 'right']].set_visible(False)\n","plt.show()\n","\n","### 2. TF-IDF Vectorization and KMeans Clustering ###\n","# Preprocess reviews\n","def preprocess_text(text):\n","    stop_words = set(stopwords.words('english'))\n","    text = re.sub(r'\\W', ' ', text)  # Remove non-word characters\n","    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n","    text = text.lower()  # Convert to lowercase\n","    words = [word for word in text.split() if word not in stop_words]\n","    return ' '.join(words)\n","\n","df_reviews['cleaned_reviews'] = df_reviews['review_content'].apply(preprocess_text)\n","\n","# TF-IDF Vectorizer\n","vectorizer = TfidfVectorizer(max_df=0.85, min_df=2, stop_words='english')\n","tfidf_matrix = vectorizer.fit_transform(df_reviews['cleaned_reviews'])\n","\n","# KMeans Clustering\n","kmeans = KMeans(n_clusters=3, random_state=42)\n","df_reviews['Cluster'] = kmeans.fit_predict(tfidf_matrix)\n","\n","print(\"\\nCluster Assignments:\")\n","print(df_reviews[['review_content', 'Cluster']])\n","\n","### 3. Topic Modeling with LDA ###\n","# Fit LDA Model\n","lda = LatentDirichletAllocation(n_components=3, random_state=42)\n","lda.fit(tfidf_matrix)\n","\n","# Display LDA Topics: Linear discriminant analysis (LDA) is an approach used in supervised machine learning\n","#to solve multi-class classification problems. LDA separates multiple classes with multiple features through data dimensionality reduction\n","def display_topics(model, feature_names, no_top_words):\n","    for topic_idx, topic in enumerate(model.components_):\n","        print(f\"Topic {topic_idx}:\")\n","        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n","\n","print(\"\\nLDA Topics:\")\n","display_topics(lda, vectorizer.get_feature_names_out(), no_top_words=5)\n","\n","### 4. Named Entity Recognition (NER) ###\n","# Load a blank spaCy model and add NER pipeline\n","nlp = spacy.blank(\"en\")\n","ner = nlp.add_pipe(\"ner\")\n","\n","# Define training data\n","TRAIN_DATA = [\n","    (\"The packaging was secure.\", {\"entities\": [(4, 13, \"CATEGORY\")]}),\n","    (\"The price was too high.\", {\"entities\": [(4, 9, \"CATEGORY\")]}),\n","    (\"The service was amazing.\", {\"entities\": [(4, 11, \"CATEGORY\")]}),\n","    (\"The shipping was fast.\", {\"entities\": [(4, 12, \"CATEGORY\")]}),\n","    (\"The quality was excellent.\", {\"entities\": [(4, 11, \"CATEGORY\")]}),\n","]\n","\n","# Add labels to NER\n","for _, annotations in TRAIN_DATA:\n","    for ent in annotations.get(\"entities\"):\n","        ner.add_label(ent[2])\n","\n","# Train the NER model\n","# Named entity recognition (NER) is a natural language processing (NLP) method that extracts information from text.\n","# NER involves detecting and categorizing important information in text known as named entities.\n","unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n","with nlp.disable_pipes(*unaffected_pipes):\n","    optimizer = nlp.begin_training()\n","    for i in range(10):  # Number of iterations\n","        random.shuffle(TRAIN_DATA)\n","        losses = {}\n","        batches = spacy.util.minibatch(TRAIN_DATA, size=2)\n","        for batch in batches:\n","            examples = [Example.from_dict(nlp.make_doc(text), annotations) for text, annotations in batch]\n","            nlp.update(examples, drop=0.5, losses=losses)\n","        print(f\"Iteration {i}: Losses {losses}\")\n","\n","# Test NER model\n","test_text = \"The packaging was secure and the service was helpful.\"\n","doc = nlp(test_text)\n","print(\"\\nNER Results:\")\n","print([(ent.text, ent.label_) for ent in doc.ents])\n"]},{"cell_type":"markdown","metadata":{"id":"5Joh6AUj9Iwm"},"source":["# BACKEND IMPLEMENTATION"]},{"cell_type":"markdown","metadata":{"id":"84IUNZMC9Qbg"},"source":["## Installing Necessary Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"exa3BWPe9L2i"},"outputs":[],"source":["!pip install fastapi uvicorn nest-asyncio pyngrok"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DUwK0JI_9Yho"},"outputs":[],"source":["#Important Libraries and Initialize Fast API\n","from fastapi import FastAPI, HTTPException\n","from pydantic import BaseModel\n","import nest_asyncio\n","from pyngrok import ngrok\n","import uvicorn\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mAwQjvjp9f01"},"outputs":[],"source":["from fastapi import FastAPI\n","from fastapi.middleware.cors import CORSMiddleware\n","from enum import Enum\n","from pyngrok import ngrok\n","from fastapi.responses import FileResponse\n","import uvicorn\n","import nest_asyncio\n","import difflib\n","import gspread\n","from google.oauth2.service_account import Credentials\n","from google.colab import auth\n","auth.authenticate_user()\n","\n","import gspread\n","from google.auth import default\n","creds, _ = default()\n","\n","gc = gspread.authorize(creds)\n","\n","\n","# !!! DO NOT DELETE CODE BELOW!!!!!\n","\n","# HERE IS THE WEBSITE TO CREATE NGROK TOKEN: https://dashboard.ngrok.com/get-started/your-authtoken\n","\n","class TeamMember(Enum):\n","    SOPHIE = 0\n","    DIVYA = 1\n","    HALEY = 2\n","    YINA = 3\n","#\n","\n","#  !!!!!! PLEASE SEE MY TEXT MESSAGE ON CREATING A TOKEN IN THE GC !!!!!!\n","# PASTE YOUR OWN TOKEN BELOW\n","team_tokens = [\n","    \"2poAIbS9btZy4wEGBn6M7hmKV1s_7HY2EY7D4huxndyyNnWeW\",  # Sophies Token-ID :0\n","    \"authtoken_2\", #divya token\n","    \"authtoken_3\", #haley toke\n","    \"authtoken_4\", #yina token\n","]\n","\n","df_reviews = pd.DataFrame({\n","    \"review_id\": [1, 2, 3, 4, 5],\n","    \"review_content\": [\n","        \"The packaging was secure and beautiful, but the price felt too high for the quality.\",\n","        \"The taste was amazing, and it arrived on time.\",\n","        \"The service was helpful, but the packaging was damaged.\",\n","        \"The shipping was fast, and the product was affordable.\",\n","        \"The quality of the item exceeded expectations, but the packaging was unattractive.\"\n","    ]\n","})\n","\n","# Combine Google Sheets data with the initial dataset\n","try:\n","    worksheet = sh.worksheet('AI Challenge Dataset')\n","    rows = worksheet.get_all_values()\n","    df = pd.DataFrame.from_records(rows)\n","\n","    df.head()\n","    #df_google_sheets = pd.DataFrame.from_records(rows)\n","    # Convert Google Sheets data to DataFrame\n","    #df_google_sheets = pd.DataFrame(rows[1:], columns=rows[0])  # First row is the header\n","\n","    # Extract only `review_id` and `review_content`\n","    df_google_sheets = df[['review_id', 'review_content']].dropna()\n","\n","    # Ensure `review_id` is numeric\n","    df_google_sheets['review_id'] = pd.to_numeric(df_google_sheets['review_id'], errors='coerce')\n","    df_google_sheets = df_google_sheets.dropna(subset=['review_id'])  # Remove rows with invalid review_id\n","    df_google_sheets['review_id'] = df_google_sheets['review_id'].astype(int)\n","\n","    # Combine with the original `df_reviews`\n","    df_reviews = pd.concat([df_reviews, df_google_sheets], ignore_index=True)\n","    print(\"Google Sheets data loaded and combined successfully.\")\n","    print(\"Google Sheets data loaded and combined successfully.\")\n","except Exception as e:\n","    print(f\"Failed to load Google Sheets data: {e}\")\n","\n","# Select the team member\n","current_member = TeamMember.SOPHIE  # Change this dynamically based on context\n","\n","# Get the selected token using the Enum value\n","selected_token = team_tokens[current_member.value]\n","\n","\n","# Set the ngrok authtoken\n","ngrok.set_auth_token(selected_token)\n","\n","# Load a trained NER model or create one\n","# Load a trained NER model or train one\n","MODEL_PATH = \"ner_model\"\n","\n","try:\n","    nlp = spacy.load(MODEL_PATH)\n","    print(\"Loaded pre-trained NER model.\")\n","except Exception as e:\n","    print(f\"Error loading model: {e}. Training a new model...\")\n","    nlp = spacy.blank(\"en\")\n","    ner = nlp.add_pipe(\"ner\")\n","\n","    TRAIN_DATA = [\n","        (\"The packaging was secure and beautiful.\", {\"entities\": [(4, 13, \"CATEGORY\")]}),\n","        (\"The price felt too high for the quality.\", {\"entities\": [(4, 9, \"CATEGORY\")]}),\n","        (\"The taste was amazing.\", {\"entities\": [(4, 9, \"CATEGORY\")]}),\n","        (\"Customer service was very helpful.\", {\"entities\": [(9, 16, \"CATEGORY\")]}),\n","        (\"Shipping was fast and reliable.\", {\"entities\": [(0, 8, \"CATEGORY\")]}),\n","        (\"The product quality exceeded expectations.\", {\"entities\": [(12, 19, \"CATEGORY\")]}),\n","    ]\n","\n","\n","    for _, annotations in TRAIN_DATA:\n","        for ent in annotations.get(\"entities\"):\n","            ner.add_label(ent[2])\n","\n","    unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n","    with nlp.disable_pipes(*unaffected_pipes):\n","        optimizer = nlp.begin_training()\n","        for i in range(10):  # Number of iterations\n","            batches = spacy.util.minibatch(TRAIN_DATA, size=2)\n","            for batch in batches:\n","                examples = [Example.from_dict(nlp.make_doc(text), annotations) for text, annotations in batch]\n","                nlp.update(examples, drop=0.5)\n","        nlp.to_disk(MODEL_PATH)  # Save model for future use\n","        print(\"New model trained and saved.\")\n","\n","# Im testing this out but using our recommendation logic (our models - item to item and basic NER model)\n","# Initialize FastAPI app\n","app = FastAPI()\n","\n","@app.get(\"/favicon.ico\")\n","async def favicon():\n","    return FileResponse(\"path/to/favicon.ico\")\n","\n","\n","# Start ngrok tunnel\n","public_url = ngrok.connect(8000)\n","print(f\"Public URL: {public_url}\")\n","\n","\n","# Extract the domain from the public URL\n","#ngrok_url = public_url.public_url  # e.g., \"https://abcd-1234.ngrok-free.app\"\n","\n","# CORS middleware\n","app.add_middleware(\n","    CORSMiddleware,\n","    allow_origins=[\"*\"],  # Dynamically set the allowed origin\n","    allow_credentials=True,\n","    allow_methods=[\"*\"],\n","    allow_headers=[\"*\"],\n",")\n","\n","# work\n","# Map frontend categories to backend model columns\n","category_mapping = {\n","    'packaging': 'rating_packaging',\n","    'price': 'rating_price',\n","    'quality': 'rating_quality',\n","    'service': 'rating_service',\n","    'shipping': 'rating_shipping',\n","    'taste': 'rating_taste'\n","}\n","\n","\n","# Data model for incoming requests\n","class RecommendationRequest(BaseModel):\n","    category: str\n","\n","# Utility functions for ML pipeline\n","def is_similar(a, b, threshold=0.8):\n","    return difflib.SequenceMatcher(None, a, b).ratio() > threshold\n","\n","def get_review_content_for_category(recommendations):\n","    relevant_reviews = []\n","    for recommendation in recommendations:\n","        for _, row in df_reviews.iterrows():\n","            doc = nlp(row[\"review_content\"])\n","            print(f\"Review: {row['review_content']}\")\n","            print(f\"Detected Entities: {[ent.text for ent in doc.ents]}\")\n","\n","            for ent in doc.ents:\n","                if ent.label_ == \"CATEGORY\" and is_similar(recommendation.lower(), ent.text.lower()):\n","                    print(f\"Matching Entity: {ent.text}\")\n","                    relevant_reviews.append(row[\"review_content\"])\n","    if not relevant_reviews:\n","        relevant_reviews.append(\"No relevant reviews found.\")\n","\n","def normalize_data(group, columns):\n","    scaler = MinMaxScaler()\n","    normalized = pd.DataFrame(scaler.fit_transform(group[columns]), columns=columns, index=group.index)\n","    return normalized\n","\n","def compute_cosine_similarity(group, numeric_columns, metric='cosine'):\n","    qualities_transposed = group[numeric_columns].T\n","    if metric == 'cosine':\n","        similarity_matrix = 1 - pairwise_distances(qualities_transposed, metric='cosine')\n","    else:\n","        raise ValueError(f\"Unsupported metric: {metric}\")\n","    similarity_df = pd.DataFrame(similarity_matrix, index=numeric_columns, columns=numeric_columns)\n","    return similarity_df\n","\n","# ML pipeline logic\n","def generate_recommendations_no_pivot(category):\n","    print(\"calling generate_recommendation\")  # Debugging log\n","\n","    # Ensure expected columns are present\n","    expected_columns = ['rating_packaging', 'rating_price', 'rating_quality', 'rating_service', 'rating_shipping', 'rating_taste']\n","    df_pivot_by_review_id = pd.DataFrame(columns=expected_columns).fillna(0)\n","\n","    # Add dummy data if the DataFrame is empty\n","    if df_pivot_by_review_id.empty:\n","        print(\"Input DataFrame is empty. Adding default data.\")\n","        df_pivot_by_review_id = pd.DataFrame({\n","            'rating_packaging': [3, 4, 5],\n","            'rating_price': [4, 5, 2],\n","            'rating_quality': [5, 2, 3],\n","            'rating_service': [3, 5, 4],\n","            'rating_shipping': [4, 3, 5],\n","            'rating_taste': [2, 4, 5],\n","        })\n","\n","    # Normalize data\n","    qualities = df_pivot_by_review_id[expected_columns]\n","    if qualities.empty:\n","        print(\"No data to normalize.\")\n","        return {\"recommendations\": [], \"review_content\": []}\n","\n","    qualities_normalized = normalize_data(qualities, columns=qualities.columns)\n","\n","    # Perform clustering\n","    kmeans = KMeans(n_clusters=2, random_state=42)\n","    df_pivot_by_review_id['Cluster'] = kmeans.fit_predict(qualities_normalized)\n","\n","    # Filter by cluster (example: Cluster 0)\n","    cluster_data = df_pivot_by_review_id[df_pivot_by_review_id['Cluster'] == 0]\n","\n","    # Compute cosine similarity\n","    similarity_matrix = cosine_similarity(qualities_normalized.T)\n","    similarity_df = pd.DataFrame(similarity_matrix, index=expected_columns, columns=expected_columns)\n","\n","    # Generate recommendations\n","    recommendations = similarity_df[category].sort_values(ascending=False).index.tolist()[:3]\n","\n","    # Fetch relevant review content\n","    review_content = get_review_content_for_category(recommendations)\n","\n","    return {\"recommendations\": recommendations, \"review_content\": review_content}\n","\n","def generate_recommendations(category):\n","    print(\"calling generate_recommendations\")  # Debugging log\n","\n","    # Pivot DataFrame\n","    expected_columns = [\"rating_packaging\", \"rating_price\", \"rating_quality\", \"rating_service\", \"rating_shipping\", \"rating_taste\"]\n","    df_pivot_by_review_id = pd.DataFrame(columns=expected_columns).fillna(0)\n","\n","    # Add dummy data if the DataFrame is empty\n","    if df_pivot_by_review_id.empty:\n","        print(\"Input DataFrame is empty. Adding default data.\")\n","        df_pivot_by_review_id = pd.DataFrame({\n","            \"review_id\": [1, 2, 3, 4, 5],\n","            \"rating_packaging\": [5, 0, 4, 2, 3],\n","            \"rating_price\": [4, 3, 5, 1, 2],\n","            \"rating_quality\": [3, 5, 0, 4, 2],\n","            \"rating_service\": [2, 4, 5, 3, 1],\n","            \"rating_shipping\": [1, 2, 3, 5, 4],\n","            \"rating_taste\": [0, 5, 4, 3, 2],\n","        })\n","\n","    # Merge review_content into df_pivot_by_review_id\n","    df_combined = pd.merge(df_pivot_by_review_id, df_reviews, on=\"review_id\", how=\"left\")\n","\n","    # Normalize data\n","    scaler = MinMaxScaler()\n","    qualities = df_combined[expected_columns]\n","    qualities_normalized = pd.DataFrame(scaler.fit_transform(qualities), columns=qualities.columns, index=qualities.index)\n","\n","    # Perform clustering\n","    kmeans = KMeans(n_clusters=2, random_state=42)\n","    df_combined[\"Cluster\"] = kmeans.fit_predict(qualities_normalized)\n","\n","    # Filter by cluster (example: Cluster 0)\n","    cluster_data = df_combined[df_combined[\"Cluster\"] == 0]\n","\n","    # Compute cosine similarity\n","    similarity_matrix = cosine_similarity(qualities_normalized.T)\n","    similarity_df = pd.DataFrame(similarity_matrix, index=expected_columns, columns=expected_columns)\n","\n","    # Generate recommendations\n","    recommendations = similarity_df[category].sort_values(ascending=False).index.tolist()[:3]\n","\n","    # Fetch relevant reviews\n","    relevant_reviews = cluster_data.loc[:, [\"review_content\"]].dropna()[\"review_content\"].tolist()\n","\n","    return {\"recommendations\": recommendations, \"review_content\": relevant_reviews}\n","\n","# Endpoint to fetch recommendations\n","@app.post(\"/recommendations/\")\n","async def get_recommendations(request: RecommendationRequest):\n","    try:\n","        category = request.category\n","        if category not in category_mapping:\n","            raise ValueError(f\"Invalid category: {category}\")\n","\n","        mapped_category = category_mapping[category]\n","        result = generate_recommendations(mapped_category)\n","\n","        if not result[\"recommendations\"]:\n","            return {\"status\": \"error\", \"message\": \"No recommendations found\"}\n","\n","        return {\n","            \"status\": \"success\",\n","            \"data\": result\n","        }\n","\n","    except ValueError as ve:\n","        print(f\"Validation Error: {ve}\")\n","        raise HTTPException(status_code=400, detail=str(ve))\n","    except Exception as e:\n","        print(f\"Error: {str(e)}\")\n","        raise HTTPException(status_code=500, detail=\"Internal Server Error\")\n","\n","\n","# work\n","#allows for multiple event loops\n","nest_asyncio.apply()\n","\n","\n","# Start the FastAPI server\n","uvicorn.run(app, host=\"0.0.0.0\", port=8000)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mHEQwUMjVka6"},"outputs":[],"source":["#from fastapi.responses import FileResponse\n","#import uvicorn\n","#import nest_asyncio\n","\n","#@app.get(\"/favicon.ico\")\n","#async def favicon():\n","#    return FileResponse(\"path/to/favicon.ico\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n68sUhO9A-AX"},"outputs":[],"source":["#importing my authtoken:\n","#from pyngrok import ngrok\n","#from pyngrok import ngrok\n","#import uvicorn\n","#import nest_asyncio\n","\n","# Replace 'YOUR_AUTH_TOKEN' with the authtoken you copied from ngrok\n","#ngrok.set_auth_token(\"2poAIbS9btZy4wEGBn6M7hmKV1s_7HY2EY7D4huxndyyNnWeW\")\n","\n","#retry running ngrok\n","# Allow multiple event loops\n","#nest_asyncio.apply()\n","\n","# Expose the FastAPI app\n","#public_url = ngrok.connect(8000)\n","#print(f\"Public URL: {public_url}\")\n","\n","# Start the FastAPI server\n","#uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FFH1n5BD-WQW"},"outputs":[],"source":["#@app.post(\"/recommendations/\")\n","#async def get_recommendations(request: RecommendationRequest):\n","#    category = request.category\n","#    print(f\"Received category: {category}\")  # Log received data\n","#    recommendations = get_recommendations_for_category(category)\n","#    if not recommendations:\n","#        raise HTTPException(status_code=404, detail=\"Category not found\")\n","#    print(f\"Returning recommendations: {recommendations}\")  # Log response\n","#    return {\"recommendations\": recommendations}"]},{"cell_type":"code","source":[],"metadata":{"id":"SEOg0_K90zxH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Looking further into API endpoint\n","#import requests\n","\n","#url = 'https://1a7b-34-16-216-174.ngrok-free.app/recommendations/'\n","#payload = {'category': 'packaging'}\n","#headers = {'Content-Type': 'application/json'}\n","\n","#response = requests.post(url, json=payload, headers=headers)\n","\n","# Print the response to debug\n","#print('Status Code:', response.status_code)\n","#print('Response JSON:', response.json())"],"metadata":{"id":"F8yg8MNNqZhB"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wheOdTNcIJAl"},"outputs":[],"source":["#Authenticate the ngrok agent\n","#ngrok config add-authtoken 2poAIbS9btZy4wEGBn6M7hmKV1s_7HY2EY7D4huxndyyNnWeW"]},{"cell_type":"markdown","metadata":{"id":"t0CD-e2H-4OZ"},"source":["# Developing Front-End code in VSCODE"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}